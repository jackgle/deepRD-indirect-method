{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59b6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199e4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from predict import extract_features\n",
    "os.sys.path.append('../../evaluation/')\n",
    "import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6055b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_dimensions = 2\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.99)\n",
    "input_path = '../../data/simulated_2d/'\n",
    "output_path = './model/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29911637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x_train = pickle.load(open(input_path+'/train/input_data.pkl', 'rb'))\n",
    "y_train = pickle.load(open(input_path+'/train/target_data.pkl', 'rb'))\n",
    "sizes_train = pickle.load(open(input_path+'/train/sizes.pkl', 'rb'))\n",
    "x_val = pickle.load(open(input_path+'/val/input_data.pkl', 'rb'))\n",
    "y_val = pickle.load(open(input_path+'/val/target_data.pkl', 'rb'))\n",
    "sizes_val = pickle.load(open(input_path+'/val/sizes.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0131fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nr = 8\n",
    "# c = 0\n",
    "# plt.figure(figsize=(3, 12), dpi=200)\n",
    "# for i in range(0, nr*2, 2):\n",
    "#     idx = np.random.choice(len(x_train))\n",
    "#     plt.subplot(nr, 2, i+1)\n",
    "#     plt.scatter(x_train[idx][:,0], x_train[idx][:,1], s=0.01, c='k', alpha=0.3)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.xlim([-4, 4])\n",
    "#     plt.ylim([-4, 4])\n",
    "#     plt.subplot(nr, 2, i+2)\n",
    "#     plt.pcolormesh(extract_features(x_train[idx]).T)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c272ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "dtype = tf.float32\n",
    "x_train = [extract_features(i) for i in x_train]\n",
    "x_train = tf.cast(np.array(x_train).squeeze(), dtype)\n",
    "x_val = [extract_features(i) for i in x_val]\n",
    "x_val = tf.cast(np.array(x_val).squeeze(), dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33ae5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optionally add reference fraction prediction\n",
    "\n",
    "# y_train = np.array(y_train)\n",
    "# y_train = np.hstack([\n",
    "#     y_train, \n",
    "#     np.array([i[0]/sum(i) for i in sizes_train]).reshape(-1, 1)\n",
    "# ])\n",
    "\n",
    "# y_val = np.array(y_val)\n",
    "# y_val = np.hstack([\n",
    "#     y_val, \n",
    "#     np.array([i[0]/sum(i) for i in sizes_val]).reshape(-1, 1)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50380b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardize outputs\n",
    "# scalery = RobustScaler()\n",
    "\n",
    "# y_train = scalery.fit_transform(y_train)\n",
    "# y_val = scalery.transform(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83bc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[..., np.newaxis]\n",
    "x_val = x_val[..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5b6000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13000, 100, 100, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79322410",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.cast(np.array(y_train), dtype)\n",
    "y_val = tf.cast(np.array(y_val), dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7ea20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mean_true, cov_true, mean_pred, cov_pred):\n",
    "    \"\"\" Computes KL divergence between two multivariate Gaussians\n",
    "    \"\"\"\n",
    "    # Get dimensionality\n",
    "    num_features = tf.cast(tf.shape(mean_pred)[-1], tf.float32)\n",
    "\n",
    "    # Compute the inverse of cov_true\n",
    "    inv_cov_true = tf.linalg.inv(cov_true)\n",
    "\n",
    "    # Compute the trace term: trace(inv_cov_true @ cov_pred)\n",
    "    trace_term = tf.linalg.trace(tf.linalg.matmul(inv_cov_true, cov_pred, transpose_a=False, transpose_b=True))\n",
    "\n",
    "    # Compute the Mahalanobis term\n",
    "    diff_mean = mean_true - mean_pred\n",
    "    diff_mean_expanded = tf.expand_dims(diff_mean, axis=-1)\n",
    "    mahalanobis_term = tf.reduce_sum(tf.linalg.matmul(inv_cov_true, diff_mean_expanded) * diff_mean_expanded, axis=-2)\n",
    "\n",
    "    # Compute the log-determinants\n",
    "    log_det_cov_pred = tf.linalg.logdet(cov_pred)\n",
    "    log_det_cov_true = tf.linalg.logdet(cov_true)\n",
    "\n",
    "    # KL divergence computation\n",
    "    kl = 0.5 * (trace_term + tf.squeeze(mahalanobis_term) - num_features + log_det_cov_true - log_det_cov_pred)\n",
    "    \n",
    "    # Return the average KL divergence over the batch dimension\n",
    "    return tf.reduce_mean(kl)\n",
    "\n",
    "# reshape correlations into an upper triangular matrix\n",
    "corr_indices = [(i, j) for i in range(n_dimensions) for j in range(i + 1, n_dimensions)]\n",
    "corr_indices = tf.constant(corr_indices, dtype=tf.int64)\n",
    "def output_to_stats(batch_vectors, n_dimensions=n_dimensions):\n",
    "    \"\"\" Converts a batch of arrays each consisting of means, std devs, and pairwise correlations\n",
    "        to separate arrays of mean vectors and covariance matrices\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(batch_vectors)[0]\n",
    "    # number of unique correlations in the upper triangular part\n",
    "    num_correlations = (n_dimensions* (n_dimensions- 1)) // 2\n",
    "    # extract statistics\n",
    "    means = batch_vectors[:, :n_dimensions]\n",
    "    std_devs = batch_vectors[:, n_dimensions:(n_dimensions*2)]\n",
    "    correlations = batch_vectors[:, (n_dimensions*2):]\n",
    "    correlations = tf.maximum(-1.0, tf.minimum(1.0, correlations))\n",
    "    # expand indices for batch\n",
    "    batch_indices = tf.reshape(tf.range(batch_size, dtype=tf.int64), (-1, 1, 1))\n",
    "    batch_indices = tf.tile(batch_indices, (1, tf.shape(corr_indices)[0], 1))\n",
    "    expanded_indices = tf.concat([batch_indices, tf.tile(tf.expand_dims(corr_indices, 0), (batch_size, 1, 1))], axis=-1)\n",
    "    # scatter correlations into the upper triangular part\n",
    "    upper_triangular = tf.scatter_nd(expanded_indices, correlations, shape=(batch_size, n_dimensions, n_dimensions))\n",
    "    upper_triangular = upper_triangular + tf.transpose(upper_triangular, perm=[0, 2, 1]) - tf.linalg.diag(tf.linalg.diag_part(upper_triangular))\n",
    "    correlation_matrix = tf.linalg.set_diag(upper_triangular, tf.ones((batch_size, n_dimensions), dtype=batch_vectors.dtype))\n",
    "    # compute the covariance matrix\n",
    "    std_devs = tf.expand_dims(std_devs, axis=-1)  # shape (batch_size, n_dimensions, 1)\n",
    "    covariance_matrices = correlation_matrix * (std_devs @ tf.transpose(std_devs, perm=[0, 2, 1]))\n",
    "    return means, covariance_matrices\n",
    "\n",
    "def kl_divergence_loss(y_true, y_pred):\n",
    "    \"\"\" Wrapper for kl_divergence that formats target and output arrays\n",
    "    \"\"\"\n",
    "#     y_true = tf.cast(scalery.inverse_transform(y_true), dtype)\n",
    "#     y_pred = tf.cast(scalery.inverse_transform(y_pred), dtype)\n",
    "    mean_true, cov_true = output_to_stats(y_true)\n",
    "    mean_pred, cov_pred = output_to_stats(y_pred)\n",
    "    return kl_divergence(mean_true, cov_true, mean_pred, cov_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b0d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 91, 91, 32)        3232      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 45, 45, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 36, 36, 64)        204864    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 18, 18, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 9, 9, 64)          409664    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 683685 (2.61 MB)\n",
      "Trainable params: 683685 (2.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=x_train.shape[1:])\n",
    "x = tf.keras.layers.Conv2D(32, kernel_size=10, activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = tf.keras.layers.Conv2D(64, kernel_size=10, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = tf.keras.layers.Conv2D(64, kernel_size=10, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(y_train.shape[1], activation='linear')(x)\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "# define loss and optimizer\n",
    "model.compile(loss = kl_divergence_loss,\n",
    "              optimizer = opt)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29c46b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model\n",
    "\n",
    "# !pip install pydot\n",
    "# !pip install graphviz\n",
    "\n",
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file=\"model.png\",\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f78f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13000, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "y_pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c43f832e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=nan>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = kl_divergence_loss(y_train, y_pred)\n",
    "kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20b57f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features:  tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "inv_cov2:  tf.Tensor(\n",
      "[[[ 7.3513975   3.844594  ]\n",
      "  [ 3.844594    3.9993315 ]]\n",
      "\n",
      " [[ 3.7192318  -0.32302928]\n",
      "  [-0.32302928  2.2530615 ]]\n",
      "\n",
      " [[ 3.774963    3.2884834 ]\n",
      "  [ 3.2884834   5.8780155 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 5.7599864  -3.2129445 ]\n",
      "  [-3.2129445   4.2565007 ]]\n",
      "\n",
      " [[ 4.5998893  -2.4814246 ]\n",
      "  [-2.4814246   3.593445  ]]\n",
      "\n",
      " [[ 1.8620318  -0.50413394]\n",
      "  [-0.50413394  2.0677228 ]]], shape=(13000, 2, 2), dtype=float32)\n",
      "trace_term:  tf.Tensor([nan nan nan ... nan nan nan], shape=(13000,), dtype=float32)\n",
      "mahalanobis_term:  tf.Tensor(\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " ...\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]], shape=(13000, 1), dtype=float32)\n",
      "log_det_cov1:  tf.Tensor([nan nan nan ... nan nan nan], shape=(13000,), dtype=float32)\n",
      "log_det_cov2:  tf.Tensor([-2.6823754 -2.1132765 -2.4314327 ... -2.6528459 -2.339108  -1.2798263], shape=(13000,), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_kl_nan(y_true, y_pred):\n",
    "    \n",
    "    mean2, cov2 = output_to_stats(y_true)\n",
    "    mean1, cov1 = output_to_stats(y_pred)\n",
    "    \n",
    "    # Get dimensionality\n",
    "    num_features = tf.cast(tf.shape(mean1)[-1], tf.float32)\n",
    "    print('num_features: ', num_features)\n",
    "    \n",
    "    # Compute the inverse of cov2\n",
    "    inv_cov2 = tf.linalg.inv(cov2)\n",
    "    print('inv_cov2: ',inv_cov2)\n",
    "    \n",
    "    # Compute the trace term: trace(inv_cov2 @ cov1)\n",
    "    trace_term = tf.linalg.trace(tf.linalg.matmul(inv_cov2, cov1, transpose_a=False, transpose_b=True))\n",
    "    print('trace_term: ',trace_term)\n",
    "    \n",
    "    # Compute the Mahalanobis term\n",
    "    diff_mean = mean2 - mean1\n",
    "    diff_mean_expanded = tf.expand_dims(diff_mean, axis=-1)\n",
    "    mahalanobis_term = tf.reduce_sum(tf.linalg.matmul(inv_cov2, diff_mean_expanded) * diff_mean_expanded, axis=-2)\n",
    "    print('mahalanobis_term: ',mahalanobis_term)\n",
    "    \n",
    "    # Compute the log-determinants\n",
    "    log_det_cov1 = tf.linalg.logdet(cov1)\n",
    "    log_det_cov2 = tf.linalg.logdet(cov2)\n",
    "    print('log_det_cov1: ',log_det_cov1)\n",
    "    print('log_det_cov2: ',log_det_cov2)\n",
    "    print('\\n')\n",
    "    \n",
    "check_kl_nan(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a517be5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722400644.813940   29314 service.cc:145] XLA service 0x7f5552cbf820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722400644.813986   29314 service.cc:153]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "I0000 00:00:1722400644.846263   29314 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813/813 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 1: val_loss did not improve from inf\n",
      "813/813 [==============================] - 14s 7ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/30\n",
      "813/813 [==============================] - ETA: 0s - loss: nan\n",
      "Epoch 2: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/30\n",
      "809/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 3: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/30\n",
      "805/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 4: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/30\n",
      "803/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 5: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/30\n",
      "807/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 6: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/30\n",
      "811/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 7: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/30\n",
      "808/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 8: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/30\n",
      "809/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 9: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/30\n",
      "804/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 10: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/30\n",
      "810/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 11: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/30\n",
      "806/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 12: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/30\n",
      "808/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 13: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/30\n",
      "804/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 14: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/30\n",
      "807/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 15: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/30\n",
      "803/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 16: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/30\n",
      "806/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 17: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/30\n",
      "812/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 18: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/30\n",
      "812/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 19: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/30\n",
      "803/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 20: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/30\n",
      "810/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 21: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/30\n",
      "808/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 22: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/30\n",
      "807/813 [============================>.] - ETA: 0s - loss: nan\n",
      "Epoch 23: val_loss did not improve from inf\n",
      "813/813 [==============================] - 4s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/30\n",
      "625/813 [======================>.......] - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define best model checkpoint\n",
    "checkpoint_path = output_path+'/model_checkpoint'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_format='tf',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# train\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    validation_data = (x_val, y_val),\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    callbacks = [checkpoint_callback],\n",
    ")\n",
    "\n",
    "with open(output_path+'/model_history.npy', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Test')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0991c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e6af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd14b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f373e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10377b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
